import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F


def load_data(file_path):
    """
    Load the CSV file and return a pandas DataFrame.
    """
    df = pd.read_csv(file_path, header=0)
    return df


def convert_to_datetime(df, date_column='date'):
    """
    Convert the specified column to datetime format.
    Returns the dataframe and the count of invalid dates.
    """
    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')
    invalid_dates = df[date_column].isna().sum()
    print(f"Number of invalid dates after conversion: {invalid_dates}")
    return df, invalid_dates


def check_duplicates(df, date_column='date'):
    """
    Check for duplicate timestamps in the date column.
    Returns the count of duplicate dates.
    """
    duplicate_dates = df[date_column].duplicated().sum()
    print(f"Number of duplicate timestamps: {duplicate_dates}")
    return duplicate_dates


def calculate_time_diff(df, date_column='date'):
    """
    Calculate time differences between consecutive timestamps.
    Prints and returns the value counts of time differences.
    """
    time_diff = df[date_column].diff().value_counts()
    print("Time differences between rows:")
    print(time_diff)
    return time_diff


def check_missing_values(df):
    """
    Check for missing values in the dataframe.
    Prints and returns columns with missing values.
    """
    missing_values = df.isnull().sum()
    missing = missing_values[missing_values > 0]
    print("Missing Values:")
    print(missing)
    return missing


def extract_time_features(df, date_column='date'):
    """
    Extract time-based features from the date column.
    Adds hour, day_of_week, week, and month columns.
    """
    df['hour'] = df[date_column].dt.hour
    df['day_of_week'] = df[date_column].dt.dayofweek
    df['week'] = df[date_column].dt.isocalendar().week
    df['month'] = df[date_column].dt.month
    return df


def get_target_columns(df, start_col='date', end_col='0T'):
    """
    Get all column names between `start_col` and `end_col`, excluding `start_col` and `end_col`.
    """
    cols = df.columns.tolist()
    start_idx = cols.index(start_col)
    end_idx = cols.index(end_col)
    target_cols = cols[start_idx + 1:end_idx]
    return target_cols


def split_data(df, target_columns, date_column):
    """
    Splits the data into train (70%), validation (10%), and test (20%) sets.
    """
    df = df.sort_values(by=date_column).reset_index(drop=True)
    total_len = len(df)
    train_end = int(total_len * 0.7)
    val_end = int(total_len * 0.8)
    
    train = df.iloc[:train_end]
    val = df.iloc[train_end:val_end]
    test = df.iloc[val_end:]
    
    return train, val, test


def normalize_data(train, val, test, features):
    """
    Normalize train, validation, and test sets using the train set's mean and standard deviation.
    """
    train_mean = train[features].mean()
    train_std = train[features].std()
    
    train[features] = (train[features] - train_mean) / train_std
    val[features] = (val[features] - train_mean) / train_std
    test[features] = (test[features] - train_mean) / train_std
    
    return train, val, test

def create_sequences(data, input_len, output_len, target_columns):
    """
    Create input-output sequences for time series forecasting.
    """
    X, y = [], []
    
    for i in range(len(data) - input_len - output_len):
        X.append(data[target_columns].iloc[i:i + input_len].values)  
        y.append(data[target_columns].iloc[i + input_len:i + input_len + output_len].values)  
    
    return np.array(X), np.array(y)

class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)
        
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

def convert_to_float32(X, y):
    """
    Convert input features (X) and target variables (y) to float32 type.
    """
    X = X.astype(np.float32)
    y = y.astype(np.float32)
    return X, y

def create_dataloaders(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=64):
    """
    Create DataLoader instances for train, validation, and test datasets.
    """
    train_dataset = TimeSeriesDataset(X_train, y_train)
    val_dataset = TimeSeriesDataset(X_val, y_val)
    test_dataset = TimeSeriesDataset(X_test, y_test)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    return train_loader, val_loader, test_loader


# Main Execution
if __name__ == "__main__":
    file_path = '/content/electricity.csv'
    date_column = 'date'
    end_column = 'OT'
    
    # Load data
    df = load_data(file_path)
    
    # Convert date column to datetime
    df, invalid_dates = convert_to_datetime(df, date_column)
    
    # Check for duplicate timestamps
    duplicate_dates = check_duplicates(df, date_column)
    
    # Calculate time differences
    time_diff = calculate_time_diff(df, date_column)
    
    # Check for missing values
    missing = check_missing_values(df)
    
    # Extract time-based features
    df = extract_time_features(df, date_column)
    
    print("\nData after processing:")
    print(df.head())
    
    # Get target columns
    target_columns = get_target_columns(df, start_col=date_column, end_col=end_column)
    
    # Split the dataset into train, validation, and test sets
    train, val, test = split_data(df, target_columns, date_column)
    
    # Extract feature columns (exclude date and target columns)
    feature_columns = [col for col in df.columns if col not in [date_column] + target_columns]
    
    # Normalize the datasets
    train, val, test = normalize_data(train, val, test, feature_columns)
    
    # Prepare data for dataloaders (create X and y for train, validation, and test)
    X_train = train[feature_columns].values
    X_val = val[feature_columns].values
    X_test = test[feature_columns].values
    
    # Extract target values (assumes your target columns are a time series to forecast)
    y_train = train[target_columns].values
    y_val = val[target_columns].values
    y_test = test[target_columns].values
    
    # Convert X and y to float32 before passing to dataloaders
    X_train, y_train = convert_to_float32(X_train, y_train)
    X_val, y_val = convert_to_float32(X_val, y_val)
    X_test, y_test = convert_to_float32(X_test, y_test)
    
    # Create dataloaders for training, validation, and test
    train_loader, val_loader, test_loader = create_dataloaders(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=64)
    
    # Testing the Dataloaders by iterating over a few batches
    print("\nTesting the DataLoader (first few batches):")
    for i, (X_batch, y_batch) in enumerate(train_loader):
        print(f"Batch {i + 1}:")
        print("X_batch shape:", X_batch.shape)
        print("y_batch shape:", y_batch.shape)
        if i == 2:  # Display first 3 batches
            break

